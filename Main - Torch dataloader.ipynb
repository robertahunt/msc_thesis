{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch import optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm#_notebook as tqdm\n",
    "\n",
    "from experiment import experiment, load_experiment\n",
    "from models.BaseModel import BaseModel\n",
    "from butterflyDataset import butterflyDataset\n",
    "from experimentUtils import get_model_called, get_loss_function_called, get_optimizer_called, get_transform_called\n",
    "\n",
    "from utils import plot, check_memory_usage, start_timer, tick, count_parameters\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline\n",
    "\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timedelta('0 days 00:00:08.129399')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.avg_epoch_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepping Experiment..\n",
      "Testing data throughput...\n",
      "<class 'PIL.Image.Image'>\n",
      "<class 'PIL.Image.Image'>\n",
      "<class 'PIL.Image.Image'>\n",
      "<PIL.Image.Image image mode=RGB size=960x640 at 0x7F3736F02DD8>\n",
      "<PIL.Image.Image image mode=RGB size=960x640 at 0x7F3736F02DD8>\n",
      "<PIL.Image.Image image mode=RGB size=960x640 at 0x7F3736F02DD8>\n",
      "<class 'PIL.Image.Image'>\n",
      "<class 'PIL.Image.Image'>\n",
      "<PIL.Image.Image image mode=RGB size=960x640 at 0x7F3736F02DD8>\n",
      "<PIL.Image.Image image mode=RGB size=960x640 at 0x7F373052F358>\n",
      "<PIL.Image.Image image mode=RGB size=960x640 at 0x7F3730466438>\n",
      "<class 'PIL.Image.Image'>\n",
      "<class 'PIL.Image.Image'>\n",
      "<PIL.Image.Image image mode=RGB size=960x640 at 0x7F3736F02DD8>\n",
      "<class 'PIL.Image.Image'>\n",
      "<PIL.Image.Image image mode=RGB size=960x640 at 0x7F3730133FD0>\n",
      "<class 'PIL.Image.Image'>\n",
      "<PIL.Image.Image image mode=RGB size=960x640 at 0x7F373030B518>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/rob/anaconda3/envs/thesis/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/rob/anaconda3/envs/thesis/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 138, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/rob/Dropbox/thesis/2. code/src/butterflyDataset.py\", line 56, in __getitem__\n    sample = self.transform(sample)\n  File \"/home/rob/anaconda3/envs/thesis/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 49, in __call__\n    img = t(img)\n  File \"/home/rob/Dropbox/thesis/2. code/src/myTransforms.py\", line 19, in __call__\n    return cv2.resize(img, self.size)\nTypeError: Expected cv::UMat for argument 'src'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-72d093ee97d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#ex.prep_experiment()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/thesis/2. code/src/experiment.py\u001b[0m in \u001b[0;36mtest_experiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Testing data throughput...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input shape: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Traceback (most recent call last):\n  File \"/home/rob/anaconda3/envs/thesis/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/rob/anaconda3/envs/thesis/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 138, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/rob/Dropbox/thesis/2. code/src/butterflyDataset.py\", line 56, in __getitem__\n    sample = self.transform(sample)\n  File \"/home/rob/anaconda3/envs/thesis/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 49, in __call__\n    img = t(img)\n  File \"/home/rob/Dropbox/thesis/2. code/src/myTransforms.py\", line 19, in __call__\n    return cv2.resize(img, self.size)\nTypeError: Expected cv::UMat for argument 'src'\n"
     ]
    }
   ],
   "source": [
    "config = dict(modelName='EvenSimplerAutoEncoder',\n",
    "              modelParams=dict(imgSize=(160,240),hidden_features=500,latent_features=100, in_channels=3),\n",
    "                 max_num_epochs = 2,\n",
    "                 suffix='_delete',\n",
    "                 opt='adam',\n",
    "                 optParams = {'lr': 0.0001},\n",
    "                 loss='mse',\n",
    "             batchSize=35,\n",
    "             earlyStopping=15,\n",
    "             sides='D', \n",
    "             transforms=[('resize', {'size': (160, 240)}), \n",
    "                         'hflip', \n",
    "                         ('colorjitter',{'contrast':0.1,'saturation':0.1}), \n",
    "                         ('affine',dict(degrees=2, scale=(0.8,1.2), fillcolor=(255,255,255))),\n",
    "                         'totensor'])\n",
    "\n",
    "ex = experiment(**config)\n",
    "#ex.prep_experiment()\n",
    "ex.test_experiment()\n",
    "ex.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8148148148148148"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.classifier_t_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No classifier improvement, stopping after 577 epochs\n",
      "tensor(0.8372, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "config = dict(modelName='BasicAutoEncoderNoMaxUnpool',\n",
    "              modelParams=dict(imgSize=(160,240),hidden_features=500,latent_features=100, in_channels=3),\n",
    "                 max_num_epochs = 300,\n",
    "                 suffix='_100lf_V_aug',\n",
    "                 opt='adam',\n",
    "                 optParams = {'lr': 0.0005},\n",
    "                 loss='mse',\n",
    "             batchSize=30,\n",
    "             earlyStopping=15,\n",
    "             sides='V', \n",
    "             transforms=[('resize', {'size': (160, 240)}), \n",
    "                         'hflip', \n",
    "                         ('colorjitter',{'contrast':0.1,'saturation':0.1}), \n",
    "                         ('affine',dict(degrees=2, scale=(0.8,1.2), fillcolor=(255,255,255))),\n",
    "                         'totensor'])\n",
    "\n",
    "ex = experiment(**config)\n",
    "#ex.prep_experiment()\n",
    "ex.test_experiment()\n",
    "ex.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025893958076448828"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(ex.y_train) == 'UNCLEAR').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No classifier improvement, stopping after 39 epochs\n",
      "tensor(0.9430, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "config = dict(modelName='VariationalAutoEncoderNoUnpool',\n",
    "              modelParams=dict(imgSize=(160,240),hidden_features=500,latent_features=100,num_samples=15, cuda=True, in_channels=3),\n",
    "                 max_num_epochs = 300,\n",
    "                 suffix='_100lf_V_aug',\n",
    "                 opt='adam',\n",
    "                 optParams = {'lr': 0.0001},\n",
    "                 loss='elbo',\n",
    "             batchSize=7,\n",
    "             earlyStopping=15,\n",
    "             sides='V', \n",
    "             transforms=[('resize', {'size': (160, 240)}), \n",
    "                         'hflip', \n",
    "                         ('colorjitter',{'contrast':0.1,'saturation':0.1}), \n",
    "                         ('affine',dict(degrees=2, scale=(0.8,1.2), fillcolor=(255,255,255))),\n",
    "                         'totensor'])\n",
    "\n",
    "ex = experiment(**config)\n",
    "#ex.prep_experiment()\n",
    "ex.test_experiment()\n",
    "ex.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
